import tiktoken
from loguru import logger
import os
import pandas as pd
from langchain.schema import Document
import faiss
import pickle
import streamlit as st
from dotenv import load_dotenv

from langchain.chains import ConversationalRetrievalChain
from langchain_community.chat_models import ChatOpenAI

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS

from langchain_community.callbacks import get_openai_callback
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

from langchain.storage import LocalFileStore
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.embeddings.cache import CacheBackedEmbeddings

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LANGCHAIN_API_KEY = os.getenv("LANGCHAIN_API_KEY")
LANGCHAIN_PROJECT = os.getenv("LANGCHAIN_PROJECT")

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

# ë¯¸ë¦¬ ì •ì˜ëœ CSV íŒŒì¼ ê²½ë¡œ
PRELOADED_CSV = "test.csv"

# í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def load_csv(file_path):
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding='latin1')

    documents = []
    for index, row in df.iterrows():
        content = ' '.join(row.values.astype(str))
        document = Document(page_content=content, metadata={'source': file_path, 'row': index})
        documents.append(document)

    return documents

def get_text(file_paths):
    doc_list = []
    for file_path in file_paths:
        doc_list.extend(load_csv(file_path))
    return doc_list

@st.cache_resource()
def embed_documents(_docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\. )", " ", ""],
        length_function=len,
    )
    
    split_docs = text_splitter.split_documents(_docs)

    cache_dir = LocalFileStore("./.cache/embeddings")

    # BGE Embedding: @Mineru
    model_name = "BAAI/bge-m3"
    model_kwargs = {
        "device": "cpu"  # í•„ìš”ì— ë”°ë¼ "cuda" ë˜ëŠ” "mps"ë¡œ ë³€ê²½ ê°€ëŠ¥
    }
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs={"normalize_embeddings": True}
    )
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(split_docs, embedding=cached_embeddings)
    return vectorstore  # vectorstoreë¥¼ ë°˜í™˜

def get_vectorstore(documents):
    return embed_documents(documents)

def get_conversation_chain(vectorstore, openai_api_key, model_selection):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_selection, temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_type='mmr', verbose=True),  # ì—¬ê¸°ì„œ as_retriever í˜¸ì¶œ
        memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True
    )
    return conversation_chain

def get_vectorstore(documents):
    return embed_documents(documents)

def get_conversation_chain(vectorstore, openai_api_key, model_selection):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=model_selection, temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_type='mmr', verbose=True),
        memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True
    )
    return conversation_chain

def main():
    st.set_page_config(page_title="ê¸°ì—…ë¬¸í™” A to Z")
    st.title("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš” A to Z")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    with st.sidebar:
        model_selection = st.selectbox(
            "Choose the language model",
            ("gpt-4", "gpt-4-turbo-preview", "gpt-3.5-turbo"),
            key="model_selection"
        )

    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    if st.session_state.processComplete is None:
        with st.spinner("ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì€ ìˆ˜ ë¶„ì´ ì†Œìš”ë˜ë‹ˆ ì»¤í”¼ í•œì” í•˜ê³  ì˜¤ì„¸ìš”. â˜•ï¸"):
            documents = get_text([PRELOADED_CSV])
            vectorstore = get_vectorstore(documents)
            st.session_state.conversation = get_conversation_chain(vectorstore, OPENAI_API_KEY, st.session_state.model_selection)
            st.session_state.processComplete = True
        st.success("ë°ì´í„° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”, ê¸°ì—…ë¬¸í™” ChatBot Betaì…ë‹ˆë‹¤. ğŸ˜Š"
                       "ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. â“"
                       "  \nì•„ì§ì€ ê±¸ìŒë§ˆ ë‹¨ê³„ì´ë‹ˆ ì˜ ë¶€íƒë“œë ¤ìš”. ğŸ‘£"
                       "ì €ëŠ” ì•ìœ¼ë¡œ ë” ì„±ì¥í•  ê±°ì—ìš”. ğŸŒ±"
        }]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    history = StreamlitChatMessageHistory(key="chat_messages")

    # Chat logic
    if query := st.chat_input("íšŒì‚¬ì œë„, ë³µë¦¬í›„ìƒ, ê·¼ë¬´í™˜ê²½ ë“± ê¶ê¸ˆí•œ ê²ƒì„ ì§ˆë¬¸í•˜ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": query})

        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            chain = st.session_state.conversation

            with st.spinner("ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                result = chain({"question": query})
                with get_openai_callback() as cb:
                    st.session_state.chat_history = result['chat_history']
                response = result['answer']
                source_documents = result['source_documents']

                st.markdown(response)
                with st.expander("ê¸°ì¡´ ìœ ì‚¬ Reference"):
                    for doc in source_documents:
                        st.markdown(doc.metadata['source'], help=doc.page_content)

        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == '__main__':
    main()